{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sophisticated-yeast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 4.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in ./opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.4.4-cp38-cp38-macosx_10_9_x86_64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in ./opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in ./opt/miniconda3/envs/tensorflow/lib/python3.8/site-packages (from nltk) (4.56.0)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.6.2 regex-2021.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing nltk package\n",
    "# sentiment analysis on twitter data\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "attached-earth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk #importing nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-allen",
   "metadata": {},
   "source": [
    "Running the above command from the Python interpreter downloads and stores the tweets locally. \n",
    "Once the samples are downloaded, they are available for your use.\n",
    "\n",
    "I will use the negative and positive tweets to train model on sentiment analysis later in the tutorial. \n",
    "The tweets with no sentiments will be used to test model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "original-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-canada",
   "metadata": {},
   "source": [
    "This will import three datasets from NLTK that contain various tweets to train and test the model:\n",
    "\n",
    "negative_tweets.json: 5000 tweets with negative sentiments\n",
    "positive_tweets.json: 5000 tweets with positive sentiments\n",
    "tweets.20150430-223406.json: 20000 tweets with no sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fifth-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-packing",
   "metadata": {},
   "source": [
    "The strings() method of twitter_samples will print all of the tweets within a dataset as strings. Setting the different tweet collections as a variable will make processing and testing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "welsh-registrar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/akaash/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-scratch",
   "metadata": {},
   "source": [
    "The punkt module is a pre-trained model that helps you tokenize words and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adult-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-personal",
   "metadata": {},
   "source": [
    "NLTK provides a default tokenizer for tweets with the .tokenized() method. Add a line to create an object that tokenizes the positive_tweets.json dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "partial-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hindu-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "attached-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/akaash/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/akaash/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalizing the data\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#wordnet is a lexical database for the English language that helps the script determine the base word and \n",
    "#the averaged_perceptron_tagger resource to determine the context of a word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "orange-missouri",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(pos_tag(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-plain",
   "metadata": {},
   "source": [
    "#From the list of tags, here is the list of the most common items and their meaning:\n",
    "\n",
    "NNP: Noun, proper, singular\n",
    "NN: Noun, common, singular or mass\n",
    "IN: Preposition or conjunction, subordinating\n",
    "VBG: Verb, gerund or present participle\n",
    "VBN: Verb, past participle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "respiratory-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "print(lemmatize_sentence(tweet_tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-today",
   "metadata": {},
   "source": [
    "The function lemmatize_sentence first gets the position tag of each token of a tweet. Within the if statement, if the tag starts with NN, the token is assigned as a noun. Similarly, if the tag starts with VB, the token is assigned as a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "familiar-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-sharing",
   "metadata": {},
   "source": [
    "This code creates a remove_noise() function that removes noise and incorporates the normalization and lemmatization mentioned in the previous section. The code takes two arguments: the tweet tokens and the tuple of stop words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "touched-reynolds",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akaash/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "chief-bulgarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "print(remove_noise(tweet_tokens[0], stop_words))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "orange-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "    \n",
    "    \n",
    "#added the code to clean the sample tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "irish-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "breeding-actor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "# most frequent words\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "impaired-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "incident-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-champion",
   "metadata": {},
   "source": [
    "Building and Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "taken-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9956666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2061.0 : 1.0\n",
      "                      :) = True           Positi : Negati =   1631.4 : 1.0\n",
      "                follower = True           Positi : Negati =     23.1 : 1.0\n",
      "                     sad = True           Negati : Positi =     18.7 : 1.0\n",
      "                     via = True           Positi : Negati =     17.9 : 1.0\n",
      "                followed = True           Negati : Positi =     16.1 : 1.0\n",
      "                    damn = True           Negati : Positi =     14.9 : 1.0\n",
      "               goodnight = True           Positi : Negati =     13.1 : 1.0\n",
      "                     ugh = True           Negati : Positi =     12.9 : 1.0\n",
      "                   didnt = True           Negati : Positi =     12.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "expensive-spare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "inappropriate-circumstances",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthetic-slave",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
